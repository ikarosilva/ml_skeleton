# Speech Detection and Filtering Implementation Plan

## 1. Executive Summary

This document outlines a plan to create a speech detection and filtering pipeline for the music recommendation system. The primary goal is to improve the quality of the training data by identifying and excluding audio files that are not music (e.g., speech, podcasts, audiobooks).

This pipeline will:
- Operate on the list of unique, canonical songs generated by the `AudioDeduplicator`.
- Use a pre-trained Voice Activity Detection (VAD) model to calculate the probability that an audio file contains speech.
- Analyze a 30-second audio segment from the center of each file.
- Skip files longer than 15 minutes, consistent with the encoder's data loading.
- Heavily leverage multicore processing for high throughput.
- Cache results in a dedicated SQLite database (`speech_cache.db`) to prevent re-computation.
- Allow the speech probability threshold for filtering to be configured by the user.

## 2. Architecture & Workflow

The speech detection process is designed to be an independent step that runs after deduplication and before training.

```
┌─────────────────────────────┐
│   Clementine Song List      │
└─────────────┬───────────────┘
              │ 1. Load all songs
┌─────────────▼───────────────┐
│     AudioDeduplicator       │
└─────────────┬───────────────┘
              │ 2. Get list of unique/canonical songs
┌─────────────▼───────────────┐
│       SpeechDetector        │
│ (New Module)                │
│ - Uses multicore processing │
│ - Caches results in SQLite  │
└─────────────┬───────────────┘
              │ 3. List of songs with speech probability
┌─────────────▼───────────────┐
│        MusicDataset         │
│ - Filters songs based on    │
│   speech_threshold          │
└─────────────┬───────────────┘
              │ 4. Clean dataset for training
┌─────────────▼───────────────┐
│       Model Training        │
└─────────────────────────────┘
```

**Workflow Steps:**

1.  **Input:** The process starts with the list of deduplicated, canonical `Song` objects produced by the `AudioDeduplicator`.
2.  **Speech Detection:** This list is passed to a new `SpeechDetector` module. The detector processes the files in parallel. For each file, it computes the probability of it being speech.
3.  **Caching:** The result for each file (speech probability and file modification time) is stored in a new `speech_cache.db` SQLite database. On subsequent runs, if the file hasn't been modified, the cached probability is used instantly, avoiding re-analysis.
4.  **Filtering:** The `MusicDataset` is initialized with the list of songs and their corresponding speech probabilities. It then filters out any song where the speech probability exceeds a configurable threshold (e.g., `> 0.5`).
5.  **Training:** The model is trained on the resulting clean dataset, which consists only of files classified as music.

## 3. Pre-trained Model for Speech Detection

To ensure high accuracy and performance, we will use a specialized, pre-trained Voice Activity Detection (VAD) model.

**Recommended Model:** `silero-vad`
- **Source:** Available on PyTorch Hub.
- **Why:** It is lightweight, fast, works directly on raw waveforms, and is highly accurate for detecting speech in various audio conditions. It is a well-regarded standard for VAD.

**Integration:** The `SpeechDetector` will use `torch.hub.load` to fetch the model.

```python
# Example of loading the Silero VAD model
model, utils = torch.hub.load(
    repo_or_dir='snakers4/silero-vad',
    model='silero_vad',
    force_reload=False
)
get_speech_timestamps, _, read_audio, _, _ = utils
```

## 4. Database Schema

A new, dedicated SQLite database will be created to cache speech detection results.

**File:** `speech_cache.db`

**Table:** `is_speech_cache`

```sql
CREATE TABLE IF NOT EXISTS is_speech_cache (
    filename TEXT PRIMARY KEY,          -- Absolute path to the audio file
    speech_probability REAL NOT NULL,   -- Probability of speech in the sample [0.0, 1.0]
    mtime REAL NOT NULL,                -- File modification time, for cache invalidation
    updated_at REAL NOT NULL            -- Timestamp of when the cache entry was last updated
);

CREATE INDEX IF NOT EXISTS idx_speech_probability ON is_speech_cache(speech_probability);
```

- `speech_probability`: A floating-point number between 0.0 and 1.0. A value of `1.0` means the 30-second sample is entirely speech; `0.0` means it is entirely non-speech.
- `mtime`: Used to automatically invalidate the cache entry if the source audio file has been modified since it was last analyzed.

## 5. New Module: `ml_skeleton/music/speech_detector.py`

This new file will contain the core logic for the speech detection pipeline.

```python
# ml_skeleton/music/speech_detector.py

import torch
import torchaudio
from pathlib import Path
from dataclasses import dataclass
import multiprocessing
from multiprocessing import Pool
from typing import List, Optional

# (Assumes Song dataclass and get_default_workers are defined elsewhere)

@dataclass
class SpeechDetectionResult:
    filename: str
    speech_probability: float

class SpeechDetector:
    """
    Analyzes audio files to detect the probability of speech using
    a pre-trained VAD model. Results are cached in an SQLite database.
    """

    def __init__(self, cache_path: str = "./speech_cache.db"):
        self.cache_path = cache_path
        self._cache_conn = self._init_db()
        self._model, self._utils = self._load_model()

    def _load_model(self):
        """Loads the Silero VAD model from PyTorch Hub."""
        model, utils = torch.hub.load(
            repo_or_dir='snakers4/silero-vad',
            model='silero_vad',
            force_reload=False
        )
        return model, utils

    def _init_db(self):
        # ... Logic to connect to SQLite DB and create the table if not exists ...
        pass

    def detect_speech_in_songs(
        self,
        songs: List[Song],
        num_workers: Optional[int] = None
    ) -> List[SpeechDetectionResult]:
        """
        Processes a list of songs in parallel to detect speech.

        Args:
            songs: A list of Song objects to analyze.
            num_workers: Number of parallel processes. Defaults to 80% of CPU cores.

        Returns:
            A list of SpeechDetectionResult objects.
        """
        if num_workers is None:
            num_workers = get_default_workers()

        with Pool(num_workers) as pool:
            results = pool.map(self._process_single_song, songs)

        return [res for res in results if res is not None]

    def _process_single_song(self, song: Song) -> Optional[SpeechDetectionResult]:
        """
        Checks cache or runs detection for a single song.
        This is the target function for the multiprocessing pool.
        """
        filepath = Path(song.filepath)
        mtime = filepath.stat().st_mtime

        # 1. Check cache first
        cached_prob = self._check_cache(song.filename, mtime)
        if cached_prob is not None:
            return SpeechDetectionResult(song.filename, cached_prob)

        # 2. Skip files that are too long
        info = torchaudio.info(filepath)
        duration = info.num_frames / info.sample_rate
        if duration > 900: # 15 minutes
            # Cache a result of 0.0 to avoid re-checking
            self._update_cache(song.filename, 0.0, mtime)
            return SpeechDetectionResult(song.filename, 0.0)

        # 3. Load center 30s of audio
        try:
            # (Logic from audio_loader.py to load center 30s of a file)
            waveform = self._load_center_audio(filepath, duration)
        except Exception:
            return None # Skip if file is corrupt or can't be loaded

        # 4. Run VAD model
        read_audio = self._utils[2] # read_audio utility
        # The model expects a 16kHz sample rate
        resampler = torchaudio.transforms.Resample(info.sample_rate, 16000)
        resampled_waveform = resampler(waveform)
        
        speech_timestamps = self._utils[0](resampled_waveform, self._model, sampling_rate=16000)
        
        total_speech_time = sum([d['end'] - d['start'] for d in speech_timestamps]) / 16000
        speech_probability = total_speech_time / 30.0 # Probability over 30s window

        # 5. Update cache
        self._update_cache(song.filename, speech_probability, mtime)

        return SpeechDetectionResult(song.filename, speech_probability)

    def _check_cache(self, filename: str, mtime: float) -> Optional[float]:
        # ... Logic to query SQLite by filename and check mtime ...
        pass

    def _update_cache(self, filename: str, probability: float, mtime: float):
        # ... Logic to INSERT or REPLACE a record in the cache ...
        pass
```

## 6. Pipeline Integration

The new `SpeechDetector` will be integrated into the main training preparation script after deduplication.

**`ml_skeleton/training/encoder_trainer.py` (Example modification):**

```python
# from ml_skeleton.music.deduplication import AudioDeduplicator
# from ml_skeleton.music.speech_detector import SpeechDetector

def prepare_training_data(config):
    songs = load_all_songs(config.music.database_path)

    # Step 1: Deduplication (existing)
    if config.music.deduplicate:
        deduplicator = AudioDeduplicator(...)
        songs = deduplicator.deduplicate(songs)

    # Step 2: Speech Detection (new)
    speech_results = {}
    if config.music.speech_detection.enabled:
        detector = SpeechDetector(config.music.speech_detection.cache_path)
        results = detector.detect_speech_in_songs(songs)
        speech_results = {res.filename: res.speech_probability for res in results}

    # Step 3: Create Dataset with filtering
    dataset = MusicDataset(
        songs,
        speech_results=speech_results, # Pass probabilities to dataset
        speech_threshold=config.music.speech_detection.speech_threshold
    )
    # ...
```

**`ml_skeleton/music/dataset.py` (Example modification):**

```python
class MusicDataset(torch.utils.data.Dataset):
    def __init__(self, songs, speech_results, speech_threshold, ...):
        self.songs = self._filter_songs(songs, speech_results, speech_threshold)
        # ...

    def _filter_songs(self, songs, speech_results, threshold):
        if not speech_results:
            return songs

        filtered_songs = []
        for song in songs:
            prob = speech_results.get(song.filename, 0.0)
            if prob <= threshold:
                filtered_songs.append(song)
        
        print(f"Filtered {len(songs) - len(filtered_songs)} songs based on speech threshold > {threshold}")
        return filtered_songs
```

## 7. Configuration

The following options will be added to `configs/music_example.yaml` to control the new pipeline.

```yaml
music:
  # ... other music settings ...

  # Speech detection and filtering
  speech_detection:
    enabled: true                          # Master switch for this feature
    cache_path: "./speech_cache.db"        # Path to the speech probability cache
    speech_threshold: 0.5                  # Filter songs with speech prob > 50%
```

This makes the feature easy to enable, disable, and configure without code changes.
